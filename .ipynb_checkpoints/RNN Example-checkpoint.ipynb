{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f07439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个cell的前向传播过程\n",
    "# 两个输入： x_t, s_prev, 以及 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ab14a-6f46-496b-bc13-2508b1de2dd9",
   "metadata": {},
   "source": [
    "Sitian:\n",
    "\n",
    "### RNN basics\n",
    "\n",
    "RNN can be used for sequence prediction, such as predicting the stock price at time T<sub>t+1</sub> given a sequence of stock prices from T<sub>0</sub>, T<sub>1</sub>, ..., T<sub>t</sub>.\n",
    "\n",
    "Once the length of the input, or say the observed sequence is determined, then the length of the unrolled RNN strucuture will also be determined. For example, if we have 5 time points in the stock price sequence, as [12, 13, 15, 16, 14], then we can 'unfold' the RNN into 5 rows, which each row takes an input, pass through a hidden unit, and give an output. The 'row' can be also called a time step. In this case, the output of a time step would also be the input of the next time step, e.g., 13 can be the output of 12 and the input of 15.  Note that the hidden unit of each time is connected with both the input and the hidden unit from the previous time, making the 5 time steps not independent from each other.\n",
    "\n",
    "A common way to signify the parameters in an RNN is as follows:  \n",
    "- x: the input of this time step\n",
    "- y: the output of this time step\n",
    "- h: the hidden state of this time step\n",
    "- h_prev: the hidden state of previous time step\n",
    "\n",
    "h is calculated as the activation (e.g., softmax) of the sum of np.dot(x, Wxh) + np.dot(h_prev, Whh) + bh, where: \n",
    "\n",
    "- Wxh: the weight matrix from an input x to N hidden units, which N is refers to as \"hidden size\" and assigned by user (e.g., 64).  \n",
    "- Whh: the weight matrix from the previous hidden state to the current hidden state.\n",
    "\n",
    "In addition, the collection of 'x' and 'y' values over time can be represented as vectors, denoted as 'X' and 'Y'. However, 'h' and 'Wxh' etc. at each time step, which are also essentially vectors, will be written in lowercase for ease of visualization.\n",
    "\n",
    "### Key process in the Algorithm \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38870c6-7fb4-486c-a7e4-5e322f3b96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]\n",
    "y = [a,b,c,d,e]\n",
    "\n",
    "\n",
    "x_train = [1,2,3,4]\n",
    "y_train = [2,3,4,5]\n",
    "\n",
    "x_pred = [1,2,3,4,5]\n",
    "y_pred = [2,3,4,5,6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a8867-9b76-4e0f-83d9-997ceadd184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43be78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一时刻的前向计算,用以得到该时刻当前 paramter 下的 h 和 output\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def time_step_forward(x_t, h_prev, parameters):\n",
    "    \"\"\"\n",
    "    This function defines the structure of the time step in an RNN at any time step.\n",
    "    \n",
    "    x_t: 当前t时刻的序列输入, 虽然只有一个值, 但为了矩阵相乘, 形状=(1, ), 即一维向量，且只有一个值\n",
    "    h_prev: 上一个时刻的 hidden state (隐层状态), a vector, 长度由hidden size确定\n",
    "    parameters: RNN share 的参数, dict. 其中各参数的形状稍有不同，但都取决于hidden size\n",
    "    \n",
    "    return: 当前隐层状态 h, 当前这层输出 out_pred\n",
    "    \"\"\"\n",
    "    \n",
    "    # 参数parameters\n",
    "    W_xh = parameters[\"W_xh\"] # 形状 (hidden_size, 1); \n",
    "    W_hy = parameters[\"W_hy\"] # 形状 (hidden_size, 1);\n",
    "    W_hh = parameters[\"W_hh\"] # 形状 (hidden_size, hidden_size);\n",
    "    b_h = parameters[\"b_h\"] # 形状 (hidden_size, ); 因为不需矩阵相乘计算，所以是一维向量\n",
    "    b_y = parameters[\"b_y\"] # 形状 (hidden_size, ); 同理, 因为不需矩阵相乘计算，所以是一维向量\n",
    "    \n",
    "    # 计算当前时刻的隐层\n",
    "    # np.dot(W_xh, x_t)中 W_hx在前，x_t在后，符合矩阵相乘的形状要求: (hidden_size, 1) dot_multiply (1, )\n",
    "    h = np.tanh(np.dot(W_xh, x_t) + np.dot(W_hh, h_prev) + b_h)\n",
    "    \n",
    "    # 计算当前时刻的 y_pred (output prediction)\n",
    "    y_pred = softmax(np.dot(h, W_hy) + b_y)\n",
    "        \n",
    "    return h, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf02ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整个 RNN (即所有时刻) 的前向计算，用以得到当前 paramter 下每一时刻的loss，用以反向传播\n",
    "\n",
    "def rnn_forward(X, Y, parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    X: 输入序列，np.array, 形状(1,T), T为序列长度, 1代表一个时刻只有一个值\n",
    "    Y: 预期输出序列 (i.e., label), np.array, 形状(1,T), T为序列长度, 1代表一个时刻只有一个值\n",
    "    parameters: 所有 time steps 的共享参数，W_xh, Why, W_hh, b_h, b_y.\n",
    "    \n",
    "    return: loss: 每一time step的 loss,用于反向传播计算\n",
    "    \"\"\"\n",
    "    # 准备记录每一time step的 loss\n",
    "    loss= []\n",
    "    h = []\n",
    "    \n",
    "    # 循环对每一个 time step 进行前向计算,记录每一个time step的loss\n",
    "    for t in range(T):\n",
    "        if t = 0:\n",
    "            h_prev = h_init\n",
    "        \n",
    "        #对于t时刻cell进行输出\n",
    "        h_prev, y_pred = time_step_forward(X[t], h_prev, parameters)\n",
    "        \n",
    "        #记录每一时刻的 h 和 loss\n",
    "        h.append(h_prev)\n",
    "        loss.append(Y[t] - y_pred)\n",
    "        \n",
    "    return h, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5cfbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一时刻的反向传播过程, 用以得到当前时刻 loss 对 parameter 的 gradient\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def time_step_backward(loss_t, h_t, parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    loss_t： 当前时刻 loss\n",
    "    parameters: 当前 parameter, 即共享参数，W_xh, Why, W_hh, b_h, b_y\n",
    "    \n",
    "    return: 当前时刻共享参数的六个 gradients: dW_xh,  \n",
    "    \"\"\"\n",
    "\n",
    "    dW_hy = np.dot(loss_t, h_t.T), \n",
    "    db_y = dy\n",
    "\n",
    "    dh = np.dot(W_hy.T, dy) + dh_next\n",
    "    dh_raw = tanh_derivative(h) * dh\n",
    "    dW_hh = np.dot(dh_raw, h_prev.T), \n",
    "    dW_xh = np.dot(dh_raw, X.T), \n",
    "    db_h = dh_raw, \n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "    #获取cache中的缓存值以及参数\n",
    "    (s_next, s_prev, x_t, parameters) = cache\n",
    "    \n",
    "    #根据公式进行反向传播计算\n",
    "    #1. 计算tanh的导数\n",
    "    dtanh = (1 - s_next ** 2) * ds_next\n",
    "    \n",
    "    #2. 计算U的导数（梯度值）\n",
    "    dU = np.dot(dtanh, x_t.T)\n",
    "    \n",
    "    #3. 计算W的梯度值\n",
    "    dW = np.dot(dtanh, s_prev.T)\n",
    "    \n",
    "    #4. 计算ba的梯度值\n",
    "    #保持计算之后U的维度不变\n",
    "    dba = np.sum(dtanh, axis = 1, keepdims = 1)\n",
    "    \n",
    "    #5. 计算x_t的导数\n",
    "    dx_t = np.dot(U.T, dtanh)\n",
    "    \n",
    "    #6. 计算s_prev的导数\n",
    "    ds_prev = np.dot(W.T, dtanh)\n",
    "    \n",
    "    #把所有的导数保存到字典当中返回\n",
    "    gradients = {\"dtanh\": dtanh, \"dU\":dU, \"dW\":dW, \"dba\": dba, \"dx_t\":dx_t, \"ds_prev\":ds_prev}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3282da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有cell的反向传播过程\n",
    "\n",
    "def rnn_backward(ds,caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    ds: 每个时刻的损失对于s的梯度值，假设是已知的(5,1,4)\n",
    "    caches: 每个cell的输出值\n",
    "    \"\"\"\n",
    "    \n",
    "    #取出caches当中的值\n",
    "    (s1, s0, x_1, parameters) = cache[0]\n",
    "    \n",
    "    #获取输入数据的总共序列长度\n",
    "    n, _, T = ds.shape\n",
    "    m, _ = x_1.shape\n",
    "    \n",
    "    # 存储所有一次更新后的参数的梯度\n",
    "    dU = np.zeros((n,m))\n",
    "    dW = np.zeros((n,n))\n",
    "    dba = np.zeros((n,1))\n",
    "    \n",
    "    # 初始化一个为0的s第二部分梯度值\n",
    "    ds_prevt = np.zeros((n,1))\n",
    "    \n",
    "    # 保存其他不需要更新的梯度\n",
    "    dx = np.zeros((m, 1, T))\n",
    "    \n",
    "    # 循环从后往前进行梯度计算\n",
    "    for t in reversed(range(T)):\n",
    "        \n",
    "        # 从3时刻开始, 2,1,0时刻的梯度由两个部分组成\n",
    "        gradients = rnn_cell_backward(ds[:, :, t] + ds_prevt, caches[t])\n",
    "        \n",
    "        ds_prevt = gradients[\"ds_prev\"]\n",
    "        \n",
    "        # U, W, ba, x_t, s_prev梯度， 共享参数需要相加\n",
    "        dU += gradients[\"dU\"]\n",
    "        dW += gradients[\"dW\"]\n",
    "        dba += gradients[\"dba\"]\n",
    "        \n",
    "        # 保存每一层的x_t, s_prev的梯度值\n",
    "        dx[:, :, t] = gradients[\"dx_t\"]\n",
    "    \n",
    "        # 返回所有更新参数的梯度以及其他变量的梯度值\n",
    "        gradients = {\"dU\": dU, \"dW\": dW, \"dba\": dba, \"dx\": dx }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b20ec-a0cb-4950-aa57-5020e19f66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calls\n",
    "\n",
    "#H_init: 初始隐层状态,可采用 Zero Initialization, 即 np.zeros(hidden_size, hidden_size)\n",
    "for i in epoch:\n",
    "    if i == 0:\n",
    "        H_prev = H_init\n",
    "    rnn_forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb34c7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array: [1 3 7 2 6 4]\n",
      "Index of the maximum value: 2\n",
      "Maximum value: 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 3, 7, 2, 6, 4])\n",
    "argmax_value = np.argmax(arr)\n",
    "\n",
    "print(\"Array:\", arr)\n",
    "print(\"Index of the maximum value:\", argmax_value)\n",
    "print(\"Maximum value:\", arr[argmax_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301b3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/text/tutorials/text_generation#download_the_shakespeare_dataset\n",
    "path_to_file = '/home/sitian/.keras/datasets/shakespeare.txt'\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e41ce0-18b3-44cd-8461-e9751b1a36f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "In mathematical terms, if you have a loss function L and the output y_t = W_hy * h_t + b_y, then by the chain rule, dW_hy = dL/dy_t * dy_t/dW_hy. The term dy_t/dW_hy is essentially h_t, the input to the weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
