{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f07439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个cell的前向传播过程\n",
    "# 两个输入： x_t, s_prev, 以及 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ab14a-6f46-496b-bc13-2508b1de2dd9",
   "metadata": {},
   "source": [
    "Sitian:\n",
    "\n",
    "### RNN basics\n",
    "\n",
    "RNN can be used for sequence prediction, such as predicting the stock price at time T<sub>t+1</sub> given a sequence of stock prices from T<sub>0</sub>, T<sub>1</sub>, ..., T<sub>t</sub>.\n",
    "\n",
    "Once the length of the input, or say the observed sequence is determined, then the length of the unrolled RNN strucuture will also be determined. For example, if we have 5 time points in the stock price sequence, as [12, 13, 15, 16, 14], then we can 'unfold' the RNN into 5 rows, which each row takes an input, pass through a hidden unit, and give an output. The 'row' can be also called a time step. In this case, the output of a time step would also be the input of the next time step, e.g., 13 can be the output of 12 and the input of 15.  Note that the hidden unit of each time is connected with both the input and the hidden unit from the previous time, making the 5 time steps not independent from each other.\n",
    "\n",
    "A common way to signify the parameters in an RNN is as follows:  \n",
    "- x: the input of this time step\n",
    "- y: the output of this time step\n",
    "- h: the hidden state of this time step\n",
    "- h_prev: the hidden state of previous time step\n",
    "\n",
    "h is calculated as the activation (e.g., softmax) of the sum of np.dot(x, Wxh) + np.dot(h_prev, Whh) + bh, where: \n",
    "\n",
    "- Wxh: the weight matrix from an input x to N hidden units, which N is refers to as \"hidden size\" and assigned by user (e.g., 64).  \n",
    "- Whh: the weight matrix from the previous hidden state to the current hidden state.\n",
    "\n",
    "In addition, the collection of 'x' and 'y' values over time can be represented as vectors, denoted as 'X' and 'Y'. However, 'h' and 'Wxh' etc. at each time step, which are also essentially vectors, will be written in lowercase for ease of visualization.\n",
    "\n",
    "### Key process in the Algorithm \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38870c6-7fb4-486c-a7e4-5e322f3b96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]\n",
    "y = [a,b,c,d,e]\n",
    "\n",
    "\n",
    "x_train = [1,2,3,4]\n",
    "y_train = [2,3,4,5]\n",
    "\n",
    "x_pred = [1,2,3,4,5]\n",
    "y_pred = [2,3,4,5,6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a8867-9b76-4e0f-83d9-997ceadd184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43be78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一时刻的前向计算,用以得到该时刻当前 paramter 下的 h 和 output\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def time_step_forward(x_t, h_prev, model_state):\n",
    "    \"\"\"\n",
    "    This function defines the structure of the time step in an RNN at any time step.\n",
    "    \n",
    "    x_t: 当前t时刻的序列输入, 虽然只有一个值, 但为了矩阵相乘, 形状=(1, ), 即一维向量，且只有一个值\n",
    "    h_prev: 上一个时刻的 hidden state (隐层状态), a vector, 长度由hidden size确定\n",
    "    model_state: RNN share 的参数, dict. 其中各参数的形状稍有不同，但都取决于hidden size\n",
    "    \n",
    "    return: 当前隐层状态 h, 当前这层输出 out_pred\n",
    "    \"\"\"\n",
    "    \n",
    "    # 参数 parameters\n",
    "    W_xh = model_state[\"W_xh\"] # 形状 (hidden_size, 1); \n",
    "    W_hy = model_state[\"W_hy\"] # 形状 (hidden_size, 1);\n",
    "    W_hh = model_states[\"W_hh\"] # 形状 (hidden_size, hidden_size);\n",
    "    b_h = model_state[\"b_h\"] # 形状 (hidden_size, ); 因为不需矩阵相乘计算，所以是一维向量\n",
    "    b_y = model_state[\"b_y\"] # 形状 (hidden_size, ); 同理, 因为不需矩阵相乘计算，所以是一维向量\n",
    "    \n",
    "    \n",
    "    # 计算当前时刻的隐层\n",
    "    # np.dot(W_xh, x_t)中 W_hx在前，x_t在后，符合矩阵相乘的形状要求: (hidden_size, 1) dot_multiply (1, )\n",
    "    h_preactivation = np.dot(W_xh, x_t) + np.dot(W_hh, h_prev) + b_h\n",
    "    h = np.tanh(h_preactivation)\n",
    "    \n",
    "    # 计算当前时刻的 y_pred (output prediction)\n",
    "    y_pred = softmax(np.dot(h, W_hy) + b_y)\n",
    "        \n",
    "    return h_preactivation, h, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf02ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整个 RNN (即所有时刻) 的前向计算，用以得到当前 paramter 下每一时刻的 loss 和 h，用以反向传播\n",
    "\n",
    "def rnn_forward(X, Y, model_state, usage = \"train\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    X: 输入序列，np.array, 形状(1,T), T为序列长度, 1代表一个时刻只有一个值\n",
    "    Y: 预期输出序列 (i.e., label), np.array, 形状(1,T), T为序列长度, 1代表一个时刻只有一个值\n",
    "    model_state: 所有 time steps 的共享参数，W_xh, Why, W_hh, b_h, b_y.\n",
    "    \n",
    "    return: loss: 每一time step的 loss,用于反向传播计算\n",
    "    \"\"\"    \n",
    "\n",
    "    \n",
    "    # 准备记录每一time step的 h_preactivation, h, y_pred, 和 loss\n",
    "    h_preactivation_all = []\n",
    "    h_all = []\n",
    "    y_pred_all = []\n",
    "    loss_all = []\n",
    "    \n",
    "    # 循环对每一个 time step 进行前向计算,记录每一个time step的loss\n",
    "    for t in range(len(X)):\n",
    "        if t = 0:\n",
    "            h_prev = model_state[\"h_init\"]\n",
    "        \n",
    "        #对于t时刻 cell 进行输出. 对于第一个 time step, 指派一个 h_init 作为其 hidden unit 的 h_prev 输入\n",
    "        h_preactivation, h_prev, y_pred = time_step_forward(X[t], h_prev, model_state)\n",
    "        \n",
    "        #记录每一时刻的 h_preactivation, h, y_pred;计算并记录 loss\n",
    "        h_preactivation_all.append(h_preactivation)\n",
    "        h_all.append(h_prev)\n",
    "        y_pred_all.append(y_pred)\n",
    "        loss_all.append(Y[t] - y_pred)\n",
    "\n",
    "    model_state.update([('h_preactivation_all', h_preactivation_all), \n",
    "                        ('h_all', h_all)])\n",
    "    if usage == \"train\":    \n",
    "        return model_state, y_pred_all, loss_all\n",
    "    elif usage == \"predict\":\n",
    "        return y_pred_all\n",
    "    else:\n",
    "        raise InputError(\"Type 'train' or 'predict'\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5cfbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一时刻的反向传播过程, 用以得到当前时刻 loss 对 parameter 的 gradient\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def time_step_backward(dy, t, model_state):\n",
    "    \n",
    "    \"\"\"    \n",
    "    return: 当前时刻共享参数的六个 gradients  \n",
    "    \"\"\"\n",
    "\n",
    "    h_prev, h, W_hh, W_xh, W_hy, h_preactivation = modle_state[\"h_all\"][t]\n",
    "    \n",
    "    dh_raw = tanh_derivative(h_preactivation) * (dh_next + np.dot(dy, W_hy.T))\n",
    "    \n",
    "    dW_hy = np.dot(loss_t, h_t.T)\n",
    "    dW_hh = np.dot(dh_raw, h_prev.T)\n",
    "    dW_xh = np.dot(dh_raw, x_t.T),\n",
    "    db_h = np.sum(dh_raw, axis=0, keepdims=True)\n",
    "    db_y = np.sum(dy, axis=0, keepdims=True)\n",
    "\n",
    "    dh_prev = np.dot(dh_raw, W_hh)\n",
    "  \n",
    "    #把所有的导数保存到字典当中返回\n",
    "    gradients = {'dW_hh': dW_hh, 'dW_xh': dW_xh, 'dW_hy': dW_hy, 'db_h': db_h, 'db_y': db_y, 'dh_prev': dh_prev}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3282da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有cell的反向传播 \n",
    "def rnn_backward(X, Y, model_state, y_pred_all, loss_all):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize gradients with zeros in the same shape as the weights/biases\n",
    "    dW_hh, dW_xh, dW_hy, db_h, db_y = np.zeros_like(model_state[\"W_hh\"]), np.zeros_likemodel_state[\"W_xh\"]), \n",
    "                                      np.zeros_like(model_state[\"W_hy\"]), np.zeros_like(model_state[\"b_h\"]), \n",
    "                                      np.zeros_like(model_state[\"b_y\"])\n",
    "\n",
    "    dh_next = np.zeros_like(model_state[\"h_all\"][0]) \n",
    "\n",
    "    # Iterate through time steps in reverse order\n",
    "    for t in reversed(range(len(X))):\n",
    "\n",
    "        dy = loss_all[t]\n",
    "        gradients = rnn_step_backward(dy, t, model_state)\n",
    "        \n",
    "        # Accumulate gradients from all time steps\n",
    "        dW_hh += gradients['dW_hh']\n",
    "        dW_xh += gradients['dW_xh']\n",
    "        dW_hy += gradients['dW_hy']\n",
    "        db_h += gradients['db_h']\n",
    "        db_y += gradients['db_y']\n",
    "        dh_next = gradients['dh_prev']  # Update dh_next for the next iteration (going backwards)\n",
    "\n",
    "    # Pack all gradients in a dictionary for return\n",
    "    all_gradients = {\n",
    "        'dW_hh': dW_hh,\n",
    "        'dW_xh': dW_xh,\n",
    "        'dW_hy': dW_hy,\n",
    "        'db_h': db_h,\n",
    "        'db_y': db_y\n",
    "    }\n",
    "\n",
    "    return all_gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb27f5f-4892-4cd0-a149-43dcd87f0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_state(model_state, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dictionary containing the parameters \"W_hh\", \"W_xh\", \"W_hy\", \"b_h\", \"b_y\"\n",
    "    gradients -- dictionary containing the gradients for each parameter\n",
    "    learning_rate -- the learning rate, scalar\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing the updated parameters\n",
    "    \"\"\"\n",
    "    # Update each parameter according to the gradient descent update rule\n",
    "    model_state['W_hh'] -= learning_rate * gradients['dW_hh']\n",
    "    model_state['W_xh'] -= learning_rate * gradients['dW_xh']\n",
    "    model_state['W_hy'] -= learning_rate * gradients['dW_hy']\n",
    "    model_state['b_h'] -= learning_rate * gradients['db_h']\n",
    "    model_state['b_y'] -= learning_rate * gradients['db_y']\n",
    "\n",
    "    return model_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b1535-c962-4632-9185-6eab9ffd3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "parameters = update_parameters(parameters, gradients, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141239d-df40-4f94-bf77-8d47eda08297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(X, Y, epochs = 20, learning_rate = 0.001):\n",
    "\n",
    "\n",
    "    # 初始化参数: W_xh, b_h, etc. 同时设定一个初始 h 作为第一个time_step 的 h_prev \n",
    "    hidden_size = 100\n",
    "    W_xh = np.random.randn(1, hidden_size)\n",
    "    W_hy = np.random.randn(hidden_size, 1)\n",
    "    W_hh = np.random.randn(hidden_size, hidden_size)\n",
    "    b_h = np.random.randn(hidden_size) # equivalent to np.random.randn(hidden_size, )\n",
    "    b_y = np.random.randn(1)\n",
    "    h_init = np.zeros(hidden_size)\n",
    "    MODEL_STATE = {\"W_xh\": W_xh, \"W_hh\": W_hh, \"W_hy\": W_hy,\n",
    "                  \"b_h\": b_h, \"b_y\": b_y, \"h_init\": h_init,\n",
    "                  \"h_preactivation_all\":None, \"h_all\": None}\n",
    "\n",
    "    for i in epochs:\n",
    "        MODEL_STATE, y_pred_all, loss_all = run_forward(X, Y, MODEL_STATE)\n",
    "        GRADIENT = rnn_backward(X, Y, MODEL_STATE, y_pred_all, loss_all)\n",
    "        MODEL_STATE = update_model_state(MODEL_STATE, GRADIENT, LEARNING_RATE)\n",
    "    \n",
    "    return MODEL_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158bb74-0412-4b72-ae9e-ca11ca6c94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, model_state):\n",
    "    run_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62acda-6954-428d-a17b-477cf81fc8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price =  = [1,2,3,4,5,6]\n",
    "X = stock_price[:-2]\n",
    "Y = stock_price[1:]\n",
    "\n",
    "MODEL_STATE = train_RNN(X, Y, epochs = 50,learning_rate = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301b3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/text/tutorials/text_generation#download_the_shakespeare_dataset\n",
    "path_to_file = '/home/sitian/.keras/datasets/shakespeare.txt'\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e41ce0-18b3-44cd-8461-e9751b1a36f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "In mathematical terms, if you have a loss function L and the output y_t = W_hy * h_t + b_y, then by the chain rule, dW_hy = dL/dy_t * dy_t/dW_hy. The term dy_t/dW_hy is essentially h_t, the input to the weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
