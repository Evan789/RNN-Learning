{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f07439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个cell的前向传播过程\n",
    "# 两个输入： x_t, s_prev, 以及 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43be78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def rnn_cell_forward(x_t, s_prev, parameters):\n",
    "    \"\"\"\n",
    "    x_t: 当前t时刻的序列输入\n",
    "    s_prev: 上一个时刻cell的隐层状态输入\n",
    "    parameters: cell中的参数\n",
    "    return: 隐层输出 s_next, 当前这层输出 out_pred\n",
    "    \"\"\"\n",
    "    \n",
    "    # 参数parameters\n",
    "    U = parameters[\"U\"]\n",
    "    W = parameters[\"W\"]\n",
    "    V = parameters[\"V\"]\n",
    "    b_a = parameters[\"b_a\"]\n",
    "    b_y = parameters[\"b_y\"]\n",
    "    \n",
    "    # 隐层输出计算\n",
    "    s_next = np.tanh(np.dot(U, x_t) + np.dot(W, s_prev) + b_a)\n",
    "    \n",
    "    # 计算cell的输出\n",
    "    out_pred = softmax(np.dot(V, s_next) + b_y)\n",
    "    \n",
    "    # 记录每一层的值，用于反向传播计算使用 \n",
    "    cache = (s_next, s_prev, x_t, U, V, W, out_pred)\n",
    "    \n",
    "    return s_next, out_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf02ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化所有cell的S，用于保存所有cell的隐层结果\n",
    "# 初始化所有cell的输出y，保存所有输出结果\n",
    "\n",
    "def rnn_forward(x, s0, parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    x: 输入序列，形状(m, 1,T), T为序列长度\n",
    "    s0: 初始状态输入\n",
    "    parameters: 所有cell的共享参数，U,W,V,ba, by\n",
    "    return: s, y, caches\n",
    "\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    \n",
    "    # 获取序列的长度，时刻数\n",
    "    m, _, T = x.shape # [3,1,4] 的shape means 3rows, 1 line each row, 4 elements each line \n",
    "    [1,10000,3]\n",
    "    \n",
    "    [[1,10000],\n",
    "     [1,10000],\n",
    "     [1,10000]]\n",
    "    \n",
    "    # 获取输入的N，定义隐层输出大小状态\n",
    "    m,n = parameters[\"V\"].shape # m?\n",
    "    \n",
    "    # 获取s0的值，保存到s_next里面，以便于前向传播传入到cell.\n",
    "    s_next = s0\n",
    "    \n",
    "    # 定义s,y并保留所有cell的隐层状态以及输出\n",
    "    s = np.zeros((n, 1, T)) \n",
    "    \n",
    "    y = np.zeros((m,1,T))\n",
    "    \n",
    "    # 循环对每一个cell进行前向传播计算\n",
    "    for t in range(T):\n",
    "        #对于t时刻cell进行输出\n",
    "        s_next, out_pred, cache = rnn_cell_forward(x[:,:,T], s_next, parameters)\n",
    "        \n",
    "        #放入数组当中\n",
    "        s[:,:,t] = s_next\n",
    "        \n",
    "        y[:,:,t] = out_pred\n",
    "\n",
    "        # 放入所有的缓存到列表当中\n",
    "        caches.append(cache)\n",
    "        \n",
    "    return s, y, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5cfbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建反向传播过程\n",
    "\n",
    "def rnn_cell_backward(ds_next, cache):\n",
    "    \n",
    "    \"\"\"\n",
    "    每个cell的右边输入梯度\n",
    "    ds_next：s_next的梯度值\n",
    "    cache: 当前cell的缓存\n",
    "    return: 该cell的六个gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    #获取cache中的缓存值以及参数\n",
    "    (s_next, s_prev, x_t, parameters) = cache\n",
    "    \n",
    "    #根据公式进行反向传播计算\n",
    "    #1. 计算tanh的导数\n",
    "    dtanh = (1 - s_next ** 2) * ds_next\n",
    "    \n",
    "    #2. 计算U的导数（梯度值）\n",
    "    dU = np.dot(dtanh, x_t.T)\n",
    "    \n",
    "    #3. 计算W的梯度值\n",
    "    dW = np.dot(dtanh, s_prev.T)\n",
    "    \n",
    "    #4. 计算ba的梯度值\n",
    "    #保持计算之后U的维度不变\n",
    "    dba = np.sum(dtanh, axis = 1, keepdims = 1)\n",
    "    \n",
    "    #5. 计算x_t的导数\n",
    "    dx_t = np.dot(U.T, dtanh)\n",
    "    \n",
    "    #6. 计算s_prev的导数\n",
    "    ds_prev = np.dot(W.T, dtanh)\n",
    "    \n",
    "    #把所有的导数保存到字典当中返回\n",
    "    gradients = {\"dtanh\": dtanh, \"dU\":dU, \"dW\":dW, \"dba\": dba, \"dx_t\":dx_t, \"ds_prev\":ds_prev}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3282da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有cell的反向传播过程\n",
    "\n",
    "def rnn_backward(ds,caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    ds: 每个时刻的损失对于s的梯度值，假设是已知的(5,1,4)\n",
    "    caches: 每个cell的输出值\n",
    "    \"\"\"\n",
    "    \n",
    "    #取出caches当中的值\n",
    "    (s1, s0, x_1, parameters) = cache[0]\n",
    "    \n",
    "    #获取输入数据的总共序列长度\n",
    "    n, _, T = ds.shape\n",
    "    m, _ = x_1.shape\n",
    "    \n",
    "    # 存储所有一次更新后的参数的梯度\n",
    "    dU = np.zeros((n,m))\n",
    "    dW = np.zeros((n,n))\n",
    "    dba = np.zeros((n,1))\n",
    "    \n",
    "    # 初始化一个为0的s第二部分梯度值\n",
    "    ds_prevt = np.zeros((n,1))\n",
    "    \n",
    "    # 保存其他不需要更新的梯度\n",
    "    dx = np.zeros((m, 1, T))\n",
    "    \n",
    "    # 循环从后往前进行梯度计算\n",
    "    for t in reversed(range(T)):\n",
    "        \n",
    "        # 从3时刻开始, 2,1,0时刻的梯度由两个部分组成\n",
    "        gradients = rnn_cell_backward(ds[:, :, t] + ds_prevt, caches[t])\n",
    "        \n",
    "        ds_prevt = gradients[\"ds_prev\"]\n",
    "        \n",
    "        # U, W, ba, x_t, s_prev梯度， 共享参数需要相加\n",
    "        dU += gradients[\"dU\"]\n",
    "        dW += gradients[\"dW\"]\n",
    "        dba += gradients[\"dba\"]\n",
    "        \n",
    "        # 保存每一层的x_t, s_prev的梯度值\n",
    "        dx[:, :, t] = gradients[\"dx_t\"]\n",
    "    \n",
    "        # 返回所有更新参数的梯度以及其他变量的梯度值\n",
    "        gradients = {\"dU\": dU, \"dW\": dW, \"dba\": dba, \"dx\": dx }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb34c7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array: [1 3 7 2 6 4]\n",
      "Index of the maximum value: 2\n",
      "Maximum value: 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 3, 7, 2, 6, 4])\n",
    "argmax_value = np.argmax(arr)\n",
    "\n",
    "print(\"Array:\", arr)\n",
    "print(\"Index of the maximum value:\", argmax_value)\n",
    "print(\"Maximum value:\", arr[argmax_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b3365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
